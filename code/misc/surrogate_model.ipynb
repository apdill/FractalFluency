{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from FF.fractal_generation import branching_network as bn\n",
    "from fracstack import measure_dimension, portfolio_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# 1. DEFINE YOUR FIXED & VARIABLE PARAMETERS\n",
    "################################################################################\n",
    "\n",
    "# Fixed \"base\" parameters that might not vary:\n",
    "neuron_params_base = {\n",
    "    'depth': 3,\n",
    "    'mean_soma_radius': 1,\n",
    "    'std_soma_radius': 1,\n",
    "    'D': 1,  \n",
    "    'branch_angle': np.pi / 4,\n",
    "    'mean_branches': 1,\n",
    "    'weave_type': 'Gauss',  \n",
    "    'randomness': 0.1,\n",
    "    'curviness': 'Gauss',\n",
    "    'curviness_magnitude': 1,\n",
    "    'n_primary_dendrites': 3,\n",
    "    'initial_thickness': 5,\n",
    "    'total_length': 500\n",
    "}\n",
    "\n",
    "network_params_base = {\n",
    "    'width': 2049,\n",
    "    'height': 2049,\n",
    "    'num_neurons': 3,\n",
    "    'edge_margin': 10,\n",
    "}\n",
    "\n",
    "param_names = [\n",
    "    'depth',\n",
    "    'D', \n",
    "    'mean_branches',\n",
    "    'n_primary_dendrites',\n",
    "    'branch_angle',\n",
    "    'total_length',\n",
    "    'num_neurons'\n",
    "]\n",
    "\n",
    "################################################################################\n",
    "# 3. DATASET GENERATION: PARAM SAMPLES -> FRACTALS -> MEASURED D\n",
    "################################################################################\n",
    "\n",
    "def latin_hypercube_sampling(param_bounds, n_samples):\n",
    "    \"\"\"\n",
    "    Latin Hypercube Sampling that guarantees inclusion of parameter bounds.\n",
    "    \n",
    "    param_bounds: List of (low, high) for each parameter dimension\n",
    "    n_samples: Number of sample points to create (must be > 2)\n",
    "    \n",
    "    Returns: Array of shape (n_samples, d) where d is number of parameters\n",
    "    \"\"\"\n",
    "    if n_samples < 3:\n",
    "        raise ValueError(\"n_samples must be at least 3 to include endpoints and middle points\")\n",
    "        \n",
    "    d = len(param_bounds)\n",
    "    samples = np.empty((n_samples, d), dtype=float)\n",
    "    \n",
    "    for i, (low, high) in enumerate(param_bounds):\n",
    "        # Create n_samples-2 points using LHS (excluding endpoints)\n",
    "        bins = np.linspace(0, 1, n_samples - 1)  # n_samples - 1 points to create n_samples - 2 bins\n",
    "        \n",
    "        # Get the lower edge of each bin\n",
    "        bin_starts = bins[:-1]\n",
    "        # Get the bin width\n",
    "        bin_width = 1.0 / (n_samples - 2)\n",
    "        \n",
    "        # Generate random offsets within each bin\n",
    "        random_offsets = np.random.rand(n_samples - 2) * bin_width\n",
    "        \n",
    "        # Final positions = bin_start + random_offset\n",
    "        positions = bin_starts + random_offsets\n",
    "        \n",
    "        # Randomly permute these positions\n",
    "        positions = np.random.permutation(positions)\n",
    "        \n",
    "        # Add endpoints (0 and 1)\n",
    "        positions = np.concatenate(([0], positions, [1]))\n",
    "        \n",
    "        # Map from [0,1] to [low,high]\n",
    "        samples[:, i] = low + positions * (high - low)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def generate_sample_fixed_params(param_vector):\n",
    "    \"\"\"\n",
    "    Generate a single sample of neuron and network parameters and measure its fractal dimension,\n",
    "    given a fixed parameter vector (one row from the LHS output).\n",
    "\n",
    "    sample_id: integer index for logging/debugging purposes.\n",
    "    param_vector: a 1D numpy array (or list) of length len(param_bounds),\n",
    "                  containing values for each parameter in the same order as param_names.\n",
    "\n",
    "    Returns:\n",
    "        (param_array, d_measured) where\n",
    "         - param_array is the final parameter values (floats, possibly rounded for int params)\n",
    "         - d_measured is the measured fractal dimension (float).\n",
    "    \"\"\"\n",
    "    # Separate sampling for neuron- and network-related parameters\n",
    "    # We'll copy them from the base dictionaries to avoid mutating global state\n",
    "    \n",
    "    neuron_sampled_params = {}\n",
    "    network_sampled_params = network_params_base.copy()\n",
    "\n",
    "    # Loop over each dimension in param_vector\n",
    "    for name, val in zip(param_names, param_vector):\n",
    "        # Round int parameters\n",
    "        if name in ['depth', 'n_primary_dendrites', 'num_neurons']:\n",
    "            val = int(round(val))\n",
    "\n",
    "        # Assign to the correct dictionary\n",
    "        if name in neuron_params_base:\n",
    "            neuron_sampled_params[name] = val\n",
    "        elif name in network_params_base:\n",
    "            network_sampled_params[name] = val\n",
    "\n",
    "    # Merge neuron parameters with defaults\n",
    "    neuron_params = neuron_params_base.copy()\n",
    "    neuron_params.update(neuron_sampled_params)\n",
    "\n",
    "    # Generate fractal & measure dimension\n",
    "    try:\n",
    "        n_tries = 0\n",
    "        generated_fractal = False\n",
    "        while not generated_fractal and n_tries < 10:\n",
    "            testnet = bn.generate_network(\n",
    "                network_id=f'nntest',\n",
    "                neuron_params=neuron_params,\n",
    "                network_params=network_sampled_params\n",
    "            )\n",
    "\n",
    "            net_masks = testnet.generate_binary_mask()\n",
    "            fractal = (net_masks['outline'] > 0).astype(np.uint8)\n",
    "\n",
    "            d_measured, _, _, _, R2 = measure_dimension(\n",
    "                fractal,\n",
    "                num_sizes=50,\n",
    "                min_size=16,\n",
    "                max_size=506,\n",
    "                num_pos=10,\n",
    "                pad_factor=1.5,\n",
    "                multiprocessing=False\n",
    "            )\n",
    "\n",
    "            if (1 < d_measured < 2) and (R2 > 0.99):\n",
    "                generated_fractal = True\n",
    "                break\n",
    "            else:\n",
    "                n_tries += 1\n",
    "                continue\n",
    "        \n",
    "        if not generated_fractal:   \n",
    "            return None\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in sample {e}\")\n",
    "        return None\n",
    "\n",
    "    # Convert parameters to array in the same order as param_names\n",
    "    # (note that param_vector might be floats; we want to return the final, possibly rounded, values)\n",
    "    final_param_array = []\n",
    "    for name, val in zip(param_names, param_vector):\n",
    "        # If integer param, use the integer we stored above\n",
    "        if name in ['depth', 'n_primary_dendrites', 'num_neurons']:\n",
    "            final_val = int(round(val))\n",
    "        else:\n",
    "            final_val = float(val)\n",
    "        final_param_array.append(final_val)\n",
    "\n",
    "    final_param_array = np.array(final_param_array, dtype=np.float32)\n",
    "\n",
    "    return (final_param_array, float(d_measured))\n",
    "\n",
    "def generate_sample(sample_id, seed):\n",
    "    \"\"\"\n",
    "    Generate a single sample of neuron and network parameters and measure its fractal dimension.\n",
    "    This function runs independently in each worker process.\n",
    "    \"\"\"\n",
    "    # Set the random seed for this worker\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Separate sampling for neuron and network parameters\n",
    "    neuron_sampled_params = {}\n",
    "    network_sampled_params = network_params_base.copy()\n",
    "\n",
    "    # Sample each parameter in param_bounds\n",
    "    for (low, high), name in zip(param_bounds, param_names):\n",
    "        val = np.random.uniform(low, high)\n",
    "\n",
    "        # Cast to int for integer parameters\n",
    "        if name in ['depth', 'n_primary_dendrites', 'num_neurons']:\n",
    "            val = int(round(val))\n",
    "\n",
    "        # Assign to appropriate dictionary\n",
    "        if name in neuron_params_base:\n",
    "            neuron_sampled_params[name] = val\n",
    "        elif name in network_params_base:\n",
    "            network_sampled_params[name] = val\n",
    "\n",
    "    # Merge neuron parameters with defaults\n",
    "    neuron_params = neuron_params_base.copy()\n",
    "    neuron_params.update(neuron_sampled_params)\n",
    "\n",
    "    # Generate fractal & measure dimension\n",
    "    try:\n",
    "        testnet = bn.generate_network(\n",
    "            network_id=f'nntest_{sample_id}',\n",
    "            neuron_params=neuron_params,\n",
    "            network_params=network_sampled_params\n",
    "        )\n",
    "        net_masks = testnet.generate_binary_mask()\n",
    "        fractal = (net_masks['outline'] > 0).astype(np.uint8)\n",
    "        d_measured, _, _, _, _ = measure_dimension(\n",
    "            fractal,\n",
    "            num_sizes=50,\n",
    "            min_size=16,\n",
    "            max_size=506,\n",
    "            num_pos=10,\n",
    "            pad_factor=2,\n",
    "            multiprocessing=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error in sample {sample_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Convert parameters to array in the same order as param_names\n",
    "    param_array = np.array([neuron_params[name] if name in neuron_params \n",
    "                           else network_sampled_params[name] \n",
    "                           for name in param_names])\n",
    "    \n",
    "    return (param_array, float(d_measured))\n",
    "\n",
    "def create_dataset_parallel(n_samples=2000, all_param_sets=None, num_workers=None):\n",
    "    \"\"\"\n",
    "    Generate a dataset of parameter sets -> measured dimension in parallel.\n",
    "    \"\"\"\n",
    "    if num_workers is None:\n",
    "        num_workers = mp.cpu_count()\n",
    "        print(f\"Using {num_workers} workers\")  # Use all available CPU cores by default\n",
    "\n",
    "    # Create a multiprocessing pool\n",
    "    with mp.Pool(processes=num_workers) as pool:\n",
    "        tasks = [all_param_sets[i] for i in range(n_samples)]\n",
    "        results = pool.map(generate_sample_fixed_params, tasks)\n",
    "\n",
    "    # Filter out any failed results (None)\n",
    "    results = [result for result in results if result is not None]\n",
    "\n",
    "    # Split the results into parameters and dimensions\n",
    "    param_samples, d_values = zip(*results)\n",
    "    print(f'% of samples failed: {100*(n_samples-len(results))/n_samples:.2f}%')\n",
    "\n",
    "    param_samples = np.array(param_samples, dtype=np.float32)\n",
    "    d_values = np.array(d_values, dtype=np.float32)\n",
    "\n",
    "    # make param_samples a dataframe with d_measured as first column\n",
    "    samples_df = pd.DataFrame(param_samples, columns=param_names)\n",
    "    samples_df.insert(0, 'd_measured', d_values)\n",
    "\n",
    "    return param_samples, d_values, samples_df\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# 4. SURROGATE MODEL: PARAMS -> D\n",
    "################################################################################\n",
    "\n",
    "class ParamToDimensionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_surrogate_model(X, y, epochs=50, lr=1e-3, batch_size=64, loss_threshold=0.01):\n",
    "    \"\"\"\n",
    "    Train a PyTorch regression model from parameter vectors X to fractal dimension y.\n",
    "    X: shape (n_samples, d_params)\n",
    "    y: shape (n_samples, )\n",
    "    \"\"\"\n",
    "    # Build dataset\n",
    "    dataset = TensorDataset(X, y)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Initialize model, loss, optimizer\n",
    "    model = ParamToDimensionModel(input_dim=X.shape[1], hidden_dim=128)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training\"):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_x, batch_y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(batch_x)\n",
    "            loss = criterion(pred, batch_y.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * len(batch_x)\n",
    "        \n",
    "        mse = total_loss / len(dataset)\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, MSE={mse:.4f}\")\n",
    "            \n",
    "        if mse < loss_threshold:\n",
    "            print(f\"Reached loss threshold {loss_threshold} at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_bounds = [\n",
    "    (3, 11),          # depth (integer)\n",
    "    (1, 2),           # D (float)\n",
    "    (0.75, 1.5),       # mean_branches (float)\n",
    "    (3, 8),           # n_primary_dendrites (integer)\n",
    "    (np.pi/8, np.pi/2),     # branch_angle (float)\n",
    "    (50, 500),      # total_length (float)\n",
    "    (3, 50)           # num_neurons (integer)\n",
    "]\n",
    "\n",
    "n_samples = 1000\n",
    "all_param_sets = latin_hypercube_sampling(param_bounds, n_samples=n_samples)\n",
    "param_names = ['depth', 'D', 'mean_branches', 'n_primary_dendrites', 'branch_angle', 'total_length', 'num_neurons']\n",
    "\n",
    "param_array, d_array, samples_df = create_dataset_parallel(\n",
    "    n_samples=n_samples,\n",
    "    all_param_sets=all_param_sets\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(15, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "params = ['D', 'depth', 'mean_branches', 'n_primary_dendrites', \n",
    "          'branch_angle', 'total_length', 'num_neurons']\n",
    "\n",
    "for i, param in enumerate(params):\n",
    "    axes[i].scatter(samples_df[param], samples_df['d_measured'], alpha=0.1)\n",
    "    axes[i].set_xlabel(f'{param}')\n",
    "    axes[i].set_ylabel('Measured D')\n",
    "    axes[i].set_title(f'{param} vs Measured D')\n",
    "    axes[i].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(np.max(samples_df['d_measured']))\n",
    "print(np.min(samples_df['d_measured']))\n",
    "\n",
    "samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = 'samples_df_1000.csv'\n",
    "samples_df.to_csv(rf'/home/apd/Projects/FractalFluency/notebooks/{df_name}', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training surrogate model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6434cf65e42b49a6a93e4d31131cd6ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300, MSE=0.4200\n",
      "Epoch 20/300, MSE=0.2986\n",
      "Epoch 30/300, MSE=0.2543\n",
      "Epoch 40/300, MSE=0.2309\n",
      "Epoch 50/300, MSE=0.2187\n",
      "Epoch 60/300, MSE=0.1995\n",
      "Epoch 70/300, MSE=0.1883\n",
      "Epoch 80/300, MSE=0.1702\n",
      "Epoch 90/300, MSE=0.1513\n",
      "Epoch 100/300, MSE=0.1424\n",
      "Epoch 110/300, MSE=0.1343\n",
      "Epoch 120/300, MSE=0.1270\n",
      "Epoch 130/300, MSE=0.1198\n",
      "Epoch 140/300, MSE=0.1129\n",
      "Epoch 150/300, MSE=0.1064\n",
      "Epoch 160/300, MSE=0.1004\n",
      "Epoch 170/300, MSE=0.0936\n",
      "Epoch 180/300, MSE=0.0873\n",
      "Epoch 190/300, MSE=0.0821\n",
      "Epoch 200/300, MSE=0.0774\n",
      "Epoch 210/300, MSE=0.0729\n",
      "Epoch 220/300, MSE=0.0684\n",
      "Epoch 230/300, MSE=0.0643\n",
      "Epoch 240/300, MSE=0.0596\n",
      "Epoch 250/300, MSE=0.0550\n",
      "Epoch 260/300, MSE=0.0498\n",
      "Epoch 270/300, MSE=0.0453\n",
      "Epoch 280/300, MSE=0.0416\n",
      "Epoch 290/300, MSE=0.0391\n",
      "Epoch 300/300, MSE=0.0366\n"
     ]
    }
   ],
   "source": [
    "# 6.2 Convert to PyTorch Tensors\n",
    "X = torch.from_numpy(param_array)        # shape: (n_samples, d_params)\n",
    "y = torch.from_numpy(d_array)            # shape: (n_samples,)\n",
    "\n",
    "# 6.3 Train the surrogate model\n",
    "print(\"Training surrogate model...\")\n",
    "model = train_surrogate_model(X, y, epochs=300, lr=1e-5, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# 5. INVERTING THE SURROGATE: TARGET D -> PARAMS (GRADIENT-BASED SEARCH)\n",
    "################################################################################\n",
    "\n",
    "def find_params_for_D(target_D, model, init_params=None, param_bounds=None,\n",
    "                      steps=500, lr=1e-2, verbose=True, loss_threshold=0.01, plot_loss=False):\n",
    "    if init_params is None:\n",
    "        # Randomly initialize within bounds\n",
    "        init_values = []\n",
    "        for (low, high) in param_bounds:\n",
    "            val = np.random.uniform(low, high)\n",
    "            init_values.append(val)\n",
    "        init_params = np.array(init_values, dtype=np.float32)\n",
    "\n",
    "\n",
    "    n_neurons = int(np.random.uniform(0.8, 1.2) * (np.round(3 + (target_D - 1.0) * (50 - 3))))\n",
    "    D = np.random.uniform(0.8, 1.2)*target_D\n",
    "\n",
    "    # Make a learnable tensor\n",
    "    params_tensor = torch.tensor(init_params, dtype=torch.float32, requires_grad=True)\n",
    "    optimizer = torch.optim.SGD([params_tensor], lr=lr)\n",
    "\n",
    "    target_tensor = torch.tensor([target_D], dtype=torch.float32)\n",
    "\n",
    "    D_param_index = 1  # Adjust these indices to match your parameter order\n",
    "    n_neurons_index = 6  # Adjust this to the correct index for n_neurons\n",
    "\n",
    "    losses = []  # Track losses if plotting requested\n",
    "\n",
    "    for step in tqdm(range(steps), desc=\"Searching\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            params_tensor[D_param_index] = D\n",
    "            params_tensor[n_neurons_index] = n_neurons\n",
    "            \n",
    "        \n",
    "        # Surrogate prediction\n",
    "        predicted_D = model(params_tensor.unsqueeze(0)).squeeze(0)\n",
    "        loss = (predicted_D - target_tensor).abs()\n",
    "        \n",
    "        if plot_loss:\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Clamp all parameters to bounds and ensure D stays at target\n",
    "        with torch.no_grad():\n",
    "            for i, (low, high) in enumerate(param_bounds):\n",
    "                if i == D_param_index:\n",
    "                    params_tensor[i] = target_D\n",
    "                elif i == n_neurons_index:\n",
    "                    params_tensor[i] = n_neurons    \n",
    "                else:\n",
    "                    params_tensor[i].clamp_(low, high)\n",
    "        \n",
    "        if verbose and step % 100 == 0:\n",
    "            print(f\"Step {step}, predicted_D={predicted_D.item():.3f}, loss={loss.item():.3f}\")\n",
    "            \n",
    "        if loss.item() < loss_threshold:\n",
    "            if verbose:\n",
    "                print(f\"Reached loss threshold {loss_threshold} at step {step}\")\n",
    "            break\n",
    "\n",
    "      \n",
    "    print(f'Loss after {step} steps: {loss.item()}')\n",
    "\n",
    "    if plot_loss:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.plot(losses)\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss vs. Optimization Step')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "    return params_tensor.detach().numpy()\n",
    "\n",
    "\n",
    "def make_portfolio(target_D_list, n_attempts=10, save_dir=None):\n",
    "    \n",
    "    found_params_list = []\n",
    "\n",
    "    for i, target_D in enumerate(target_D_list):\n",
    "\n",
    "        generated_fractal = False\n",
    "        attempts = 0\n",
    "\n",
    "\n",
    "        np.random.seed(i)\n",
    "\n",
    "        current_params = [\n",
    "                        np.random.randint(3, 11),          # depth (integer)\n",
    "                        np.random.uniform(1, 2),           # D (float)\n",
    "                        np.random.uniform(0.75, 1.5),       # mean_branches (float)\n",
    "                        np.random.randint(3, 8),           # n_primary_dendrites (integer)\n",
    "                        np.random.uniform(np.pi/8, np.pi/2),     # branch_angle (float)\n",
    "                        np.random.randint(50, 500),      # total_length (float)\n",
    "                        np.random.randint(3, 30)           # num_neurons (integer)\n",
    "                        ]   \n",
    "        \n",
    "        while generated_fractal is False and attempts < n_attempts:\n",
    "\n",
    "            found_params = find_params_for_D(\n",
    "                target_D=target_D,\n",
    "                model=model,\n",
    "                init_params=current_params, \n",
    "                param_bounds=param_bounds,\n",
    "                steps=50000,\n",
    "                lr=1e-4,\n",
    "                verbose=False,\n",
    "                plot_loss=False,\n",
    "                loss_threshold=0.01\n",
    "            )\n",
    "\n",
    "            neuron_params = neuron_params_base.copy()\n",
    "            network_params = network_params_base.copy()\n",
    "\n",
    "            for name, val in zip(param_names, found_params):\n",
    "                if name in neuron_params_base:\n",
    "                    neuron_params[name] = float(val)\n",
    "                else:\n",
    "                    network_params[name] = float(val)\n",
    "        \n",
    "            # Ensure integer values for certain parameters\n",
    "            for param in ['depth', 'n_primary_dendrites', 'num_neurons']:\n",
    "                if param in neuron_params:\n",
    "                    neuron_params[param] = int(round(neuron_params[param]))\n",
    "                if param in network_params:\n",
    "                    network_params[param] = int(round(network_params[param]))\n",
    "\n",
    "            branch_network = bn.generate_network(network_id=f'nntest_{target_D}', neuron_params=neuron_params, network_params=network_params)\n",
    "            net_masks = branch_network.generate_binary_mask()\n",
    "            fractal = (net_masks['outline'] > 0).astype(np.uint8)\n",
    "            d_value, _, _, _, R2 = measure_dimension(fractal, num_sizes=50, min_size=16, max_size=506, num_pos=100, pad_factor=2, multiprocessing=True)\n",
    "\n",
    "            if np.abs(d_value - target_D) < 0.025:\n",
    "                if R2 > 0.99:\n",
    "                    generated_fractal = True\n",
    "                    found_params_list.append(found_params)\n",
    "                    break\n",
    "                else:\n",
    "                    attempts += 1\n",
    "                    print(f'R2={R2} is too low for target_D={target_D}')\n",
    "            else:\n",
    "                attempts += 1\n",
    "                print(f'd_value={d_value} is too far from target_D={target_D}')\n",
    "        \n",
    "        if not generated_fractal:\n",
    "            print(f'Failed to generate fractal for target_D={target_D}')\n",
    "            continue\n",
    "\n",
    "        portfolio_plot(fractal, target_D0=target_D, save_dir=save_dir, f_name=f'branch_network_example_{target_D}.png')\n",
    "\n",
    "    \n",
    "    params_df = pd.DataFrame(found_params_list, columns=param_names)\n",
    "    params_df.insert(0, 'target_D', target_D_list)    \n",
    "\n",
    "    return params_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding params for target D=1.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8920aa7e2aec4908bfd8a059e2f790d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Searching:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 19999 steps: 0.33663511276245117\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depth</th>\n",
       "      <th>D</th>\n",
       "      <th>mean_branches</th>\n",
       "      <th>n_primary_dendrites</th>\n",
       "      <th>branch_angle</th>\n",
       "      <th>total_length</th>\n",
       "      <th>num_neurons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.373749</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.75</td>\n",
       "      <td>5.417548</td>\n",
       "      <td>1.570796</td>\n",
       "      <td>81.318222</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       depth    D  mean_branches  n_primary_dendrites  branch_angle  \\\n",
       "0  10.373749  1.9           0.75             5.417548      1.570796   \n",
       "\n",
       "   total_length  num_neurons  \n",
       "0     81.318222         43.0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6.4 Invert for a single target D\n",
    "target_D = 1.9\n",
    "print(f\"\\nFinding params for target D={target_D}\")\n",
    "\n",
    "found_params = find_params_for_D(\n",
    "    target_D=target_D,\n",
    "    model=model,\n",
    "    init_params=None,    # or pass a custom init\n",
    "    param_bounds=param_bounds,\n",
    "    steps=20000,\n",
    "    lr=1e-3,\n",
    "    verbose=False,\n",
    "    plot_loss=False,\n",
    "    loss_threshold=0.05\n",
    ")\n",
    "\n",
    "found_params_df = pd.DataFrame([found_params], columns=param_names)\n",
    "found_params_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = r'/home/apd/Projects/FractalFluency/datasets/fractal_portfolio/branch_networks'\n",
    "\n",
    "target_D_list = np.linspace(1.1, 1.9, 9)\n",
    "\n",
    "make_portfolio(target_D_list=target_D_list, n_attempts=10, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.5 Generate fractals across a range from D=1.1 to D=1.9 (smooth walk)\n",
    "target_Ds = np.linspace(1.1, 1.9, 9)\n",
    "print(\"\\nGenerating smooth range of fractals from D=1.1 to D=1.9...\")\n",
    "current_params = None\n",
    "generated_params_list = []\n",
    "\n",
    "for d_target in target_Ds:\n",
    "    print(f\"Target D={d_target:.2f}\")\n",
    "    current_params = find_params_for_D(\n",
    "        target_D=d_target,\n",
    "        model=model,\n",
    "        init_params=current_params,\n",
    "        param_bounds=param_bounds,\n",
    "        steps=50000,\n",
    "        lr=1e-3,\n",
    "        verbose=False,\n",
    "        plot_loss=False,\n",
    "        loss_threshold=0.05\n",
    "    )\n",
    "    generated_params_list.append(current_params)\n",
    "\n",
    "# 6.6 Create fractals using these found params and measure real D\n",
    "for i, d_target in enumerate(target_Ds):\n",
    "    solution_params = generated_params_list[i]\n",
    "    \n",
    "    # Build separate dicts for neuron and network params\n",
    "    neuron_params = neuron_params_base.copy()\n",
    "    network_params = network_params_base.copy()\n",
    "    \n",
    "    # Assign each parameter to the correct dictionary\n",
    "    for name, val in zip(param_names, solution_params):\n",
    "        if name in neuron_params_base:\n",
    "            neuron_params[name] = float(val)\n",
    "        else:\n",
    "            network_params[name] = float(val)\n",
    "    \n",
    "    # Ensure integer values for certain parameters\n",
    "    for param in ['depth', 'n_primary_dendrites']:\n",
    "        if param in neuron_params:\n",
    "            neuron_params[param] = int(round(neuron_params[param]))\n",
    "    network_params['num_neurons'] = int(round(network_params['num_neurons']))\n",
    "    \n",
    "    neuronn = bn.generate_network(\n",
    "        network_id=f'nntest_{i}', \n",
    "        neuron_params=neuron_params, \n",
    "        network_params=network_params\n",
    "    )\n",
    "    net_masks = neuronn.generate_binary_mask()\n",
    "    fractal = (net_masks['outline'] > 0).astype(np.uint8)\n",
    "    measured_d, _, _, _, _ = measure_dimension(fractal, num_sizes=50, min_size=16, max_size=506, num_pos=100, pad_factor=2, multiprocessing=True)\n",
    "    print(f\"---\\nFor target D={d_target:.2f}, measured D={measured_d:.3f}\")\n",
    "    # Save or display fractal if desired, e.g., plt.imshow(fractal_img)...\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
